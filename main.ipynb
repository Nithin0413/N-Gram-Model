{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d3918-4516-466f-bbb1-1c5a2d8ce6a7",
   "metadata": {},
   "source": [
    "<h1></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff3459-971d-412c-b3c8-0ef9da620756",
   "metadata": {},
   "source": [
    "**Unigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcc17b1-b50a-4cd0-8cfe-2bcc02d19dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity =  inf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "#TRAINING\n",
    "# Step 1 - accesing the training data \n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "\n",
    "# Step 2 - remove all punctuatuons from the text corpus\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "\n",
    "# Step 3 - split the text by words (since it's unigram, spliting by sentences is not required)\n",
    "train_words=train_text_corpus.split()\n",
    "\n",
    "# Step 4 - construct a dictionary of words as key and count as value - unigram count\n",
    "unigram_dictionary = {}\n",
    "for token in train_words:\n",
    "    unigram_dictionary[token] = unigram_dictionary.get(token,0) + 1\n",
    "\n",
    "\n",
    "# Step 5 - constrcut a probability distribution from the training data\n",
    "total_unigrams = len(train_words)\n",
    "unigram_probability = {}\n",
    "for key,value in unigram_dictionary.items():\n",
    "    unigram_probability[key] = (value)/(total_unigrams)\n",
    "\n",
    "#TESTING\n",
    "# Step 6 - open the testing data \n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "# Step 7 - remove all punctuations\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "\n",
    "# Step 8 - split the text by sentences\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "# Step 9 - evaluating perflexity\n",
    "log_probability =0.0\n",
    "total_test_words=0\n",
    "for sentence in test_sentences:\n",
    "    words_sentence = sentence.split()\n",
    "    probability_sentence=0.0\n",
    "    for token in words_sentence:\n",
    "        total_test_words+=1\n",
    "        if token in unigram_probability:\n",
    "            probability_sentence += math.log(unigram_probability[token],2)\n",
    "        else:\n",
    "            probability_sentence +=float(\"-inf\")\n",
    "    log_probability  += probability_sentence\n",
    "perlexity = 2**(-log_probability/total_test_words)\n",
    "print(\"Perplexity = \",perlexity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8f93d02-1865-46e7-b816-7ea9087bbb72",
   "metadata": {},
   "source": [
    "The perplexity of the unigram model becomes inifity due to the presence of out of vocabulary words in the test data which is not present in the training data as the probabillity becomes for such sentences contianing out of vocabulary words as there is no smoothin done in the naive implementation to handle the out of vocabulary words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52fa9fa3-d79d-4b13-b258-2fbde8c87202",
   "metadata": {},
   "source": [
    "The code represents a basic unigram language model. In the training phase, it reads text data, removes punctuation, and calculates unigram probabilities. In the testing phase, it assesses the model's perplexity on a separate dataset, which measures how well the model predicts the test data. Perplexity is lower for better models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ac8ab-daed-4dfe-ad64-915baa32f5fd",
   "metadata": {},
   "source": [
    "**Bigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e48c137-ae34-4b5c-8653-893855ad4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity = inf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "#TRAINING\n",
    "# Step 1 - accesing the training data \n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "\n",
    "# Step 2 - remove all punctuatuons from the text corpus\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "\n",
    "# Step 3 - split the text by sentences\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "# Step 4 - adding the start and stop symbols\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = \"<s> \" + train_sentences[i] + \" </s>\"\n",
    "\n",
    "# Step 5 - construct a dictionary of bigram\n",
    "unigram_dictionary ={}\n",
    "bigram_dictionary ={}\n",
    "total_bigram=0\n",
    "total_unigram=0\n",
    "for sentence in train_sentences:\n",
    "    words_sentence =sentence.split()\n",
    "    len_sen = len(words_sentence)\n",
    "    for i in range(len_sen-1):\n",
    "        bigram =[]\n",
    "        # unigram dictionary\n",
    "        unigram_dictionary[words_sentence[i]]=unigram_dictionary.get(words_sentence[i],0) +1\n",
    "        # biigram dictionary\n",
    "        for j in range(2):\n",
    "            bigram.append(words_sentence[i+j])\n",
    "        bigram_tuple=tuple(bigram)\n",
    "        bigram_dictionary[bigram_tuple]=bigram_dictionary.get(bigram_tuple,0) +1\n",
    "        total_bigram+=1\n",
    "\n",
    "# Step 6 - construct a probability dirtibution for bigram\n",
    "bigram_probability={}\n",
    "for key,value in bigram_dictionary.items():\n",
    "    bigram_dictionary[key]=(value)/(unigram_dictionary[key[0]])\n",
    "\n",
    "#TESTING\n",
    "#Step 7 - open the testing data\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "# Step 7 - remove all punctuations\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "\n",
    "# Step 8 - split the text by sentences\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "# Step 9 - add start and stop symbols\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = \"<s> \" + test_sentences[i] + \" </s>\"\n",
    "\n",
    "# Step 10 - evaluating perflexity\n",
    "log_probability =0.0\n",
    "total_test_words=0\n",
    "for sentence in test_sentences:\n",
    "    words_sentence = sentence.split()\n",
    "    probability_sentence=0.0\n",
    "    len_sen = len(words_sentence)\n",
    "    total_test_words+=len_sen\n",
    "    for i in range(len_sen-1):\n",
    "        bigram =[]\n",
    "        for j in range(2):\n",
    "            bigram.append(words_sentence[i+j])\n",
    "        bigram_tuple=tuple(bigram)\n",
    "        if bigram_tuple in bigram_dictionary:\n",
    "            probability_sentence += math.log(bigram_dictionary[bigram_tuple]/unigram_dictionary[bigram_tuple[0]],2)\n",
    "        else:\n",
    "            probability_sentence+=float(\"-inf\")\n",
    "    log_probability+=probability_sentence\n",
    "perlexity = 2**(-log_probability/total_test_words)\n",
    "print(\"Perplexity =\",perlexity)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5badc301-a45e-43ae-9494-63d07e3b9038",
   "metadata": {},
   "source": [
    "Same as above reason, since it's a naive model without smoothing, out of vocalbulary words makes the perplexity infinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c619ee8-c609-44ad-bf55-6956cc13505e",
   "metadata": {},
   "source": [
    "The code implements a bigram language model. During training, it processes text data by removing punctuation, splitting it into sentences, adding start and stop symbols, and constructing a bigram model. The bigram model calculates probabilities based on the training data. During testing, the code evaluates the perplexity of the bigram model on a separate dataset, measuring its predictive accuracy. Lower perplexity values indicate better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f78779-9212-4f78-8f11-9dab08e2ecc2",
   "metadata": {},
   "source": [
    "**Trigram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76c9342-b08f-4c9d-9a54-e45d7c3d854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity =  inf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "#TRAINING\n",
    "# Step 1 - accesing the training data \n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "\n",
    "# Step 2 - remove all punctuatuons from the text corpus\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "\n",
    "# Step 3 - split the text by sentences\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "# Step 4 - adding the start and stop symbols\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = \"<s> \" + train_sentences[i] + \" </s>\"\n",
    "\n",
    "# Step 5 - construct a dictionary of trigram and bigram\n",
    "unigram_dictionary ={}\n",
    "bigram_dictionary ={}\n",
    "trigram_dictionary ={}\n",
    "total_bigram =0\n",
    "total_trigram =0\n",
    "for sentence in train_sentences:\n",
    "    words_sentence =sentence.split()\n",
    "    len_sen = len(words_sentence)\n",
    "    # unigram dictionary\n",
    "    for i in range(len_sen-1):\n",
    "        unigram_dictionary[words_sentence[i]]=unigram_dictionary.get(words_sentence[i],0) +1\n",
    "    # bigram dictionary\n",
    "    for i in range(len_sen-1):\n",
    "        bigram =[]\n",
    "        for j in range(2):\n",
    "            bigram.append(words_sentence[i+j])\n",
    "        bigram_tuple=tuple(bigram)\n",
    "        bigram_dictionary[bigram_tuple]=bigram_dictionary.get(bigram_tuple,0) +1\n",
    "        total_bigram+=1\n",
    "    #trigram dictionary\n",
    "    for i in range(len_sen-2):\n",
    "        trigram =[]\n",
    "        for j in range(3):\n",
    "            trigram.append(words_sentence[i+j])\n",
    "        trigram_tuple=tuple(trigram)\n",
    "        trigram_dictionary[trigram_tuple]=trigram_dictionary.get(trigram_tuple,0) +1\n",
    "        total_trigram+=1\n",
    "\n",
    "# Step 7 - probability distribution of trigram\n",
    "trigram_probability={}\n",
    "for key,value in trigram_dictionary.items():\n",
    "    trigram_probability[key] =(value)/(bigram_dictionary[key[0:2]])\n",
    "\n",
    "\n",
    "#TESTING\n",
    "#Step 7 - open the testing data\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "# Step 8 - remove all punctuations\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "\n",
    "# Step 9 - split the text by sentences\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "# Step 10 - add start and stop symbols\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = \"<s> \" + test_sentences[i] + \" </s>\"\n",
    "\n",
    "# Step 11 - evaluating perplexity\n",
    "log_probability =0.0\n",
    "total_test_words=0\n",
    "for sentence in test_sentences:\n",
    "    words_sentence = sentence.split()\n",
    "    probability_sentence=0.0\n",
    "    len_sen = len(words_sentence)\n",
    "    total_test_words+=len_sen\n",
    "    for i in range(len_sen-2):\n",
    "        trigram =[]\n",
    "        for j in range(3):\n",
    "            trigram.append(words_sentence[i+j])\n",
    "            trigram_tuple=tuple(trigram)\n",
    "        bigram_tuple=trigram_tuple[0:2]\n",
    "        if trigram_tuple in trigram_dictionary:\n",
    "            probability_sentence += math.log(trigram_dictionary[trigram_tuple]/bigram_dictionary[bigram_tuple],2)\n",
    "        else:\n",
    "            probability_sentence+=float(\"-inf\")\n",
    "    log_probability+=probability_sentence\n",
    "perlexity = 2**(-log_probability/total_test_words)\n",
    "print(\"Perplexity = \",perlexity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24d68413-a0f4-4523-899f-52f92a6f400e",
   "metadata": {},
   "source": [
    "As discussed above, since we are not implementing the out of vocabulary words in this question, perplexity becomes infinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80984f7e-4ff9-41ba-80f4-1d7ee43ccc36",
   "metadata": {},
   "source": [
    "The code builds a trigram language model and calculates perplexity for text data. In the training phase, it processes training text, removes punctuation, adds start and stop symbols, and constructs dictionaries for unigrams, bigrams, and trigram. In the testing phase, it evaluates the trigram model on test text, calculates the perplexity by measuring how well the model predicts the test data, and prints the resulting perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f60be-578b-4924-8a22-087ab0096664",
   "metadata": {},
   "source": [
    "<h1><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbbec0-e28a-4ffa-a7f0-06c7abb81158",
   "metadata": {},
   "source": [
    "**Smoothing in unigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca4988a2-1e00-492f-9b1d-bbe6e9e37758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k =  0.01  perplexity =  1848.7297844969935\n",
      "for k =  0.02  perplexity =  1823.6577201265968\n",
      "for k =  0.03  perplexity =  1809.253208354949\n",
      "for k =  0.04  perplexity =  1799.1764441662845\n",
      "for k =  0.05  perplexity =  1791.4576978759405\n",
      "for k =  0.060000000000000005  perplexity =  1785.224571633606\n",
      "for k =  0.07  perplexity =  1780.0137007290348\n",
      "for k =  0.08  perplexity =  1775.5494561041862\n",
      "for k =  0.09  perplexity =  1771.6545661169991\n",
      "for k =  0.09999999999999999  perplexity =  1768.208290285365\n",
      "for k =  0.10999999999999999  perplexity =  1765.124693909892\n",
      "for k =  0.11999999999999998  perplexity =  1762.340444956167\n",
      "for k =  0.12999999999999998  perplexity =  1759.8075277459354\n",
      "for k =  0.13999999999999999  perplexity =  1757.4886733208311\n",
      "for k =  0.15  perplexity =  1755.3543753555755\n",
      "for k =  0.16  perplexity =  1753.3808743298232\n",
      "for k =  0.17  perplexity =  1751.5487559681733\n",
      "for k =  0.18000000000000002  perplexity =  1749.8419522687138\n",
      "for k =  0.19000000000000003  perplexity =  1748.2470139178404\n",
      "for k =  0.20000000000000004  perplexity =  1746.752570197723\n",
      "for k =  0.21000000000000005  perplexity =  1745.3489212647612\n",
      "for k =  0.22000000000000006  perplexity =  1744.027725695704\n",
      "for k =  0.23000000000000007  perplexity =  1742.7817577911505\n",
      "for k =  0.24000000000000007  perplexity =  1741.6047167502707\n",
      "for k =  0.25000000000000006  perplexity =  1740.4910749632384\n",
      "for k =  0.26000000000000006  perplexity =  1739.4359561806368\n",
      "for k =  0.2700000000000001  perplexity =  1738.4350367679592\n",
      "for k =  0.2800000000000001  perplexity =  1737.4844649902518\n",
      "for k =  0.2900000000000001  perplexity =  1736.5807945158642\n",
      "for k =  0.3000000000000001  perplexity =  1735.720929236914\n",
      "for k =  0.3100000000000001  perplexity =  1734.9020771714527\n",
      "for k =  0.3200000000000001  perplexity =  1734.1217117120273\n",
      "for k =  0.3300000000000001  perplexity =  1733.3775388587885\n",
      "for k =  0.34000000000000014  perplexity =  1732.6674693618952\n",
      "for k =  0.35000000000000014  perplexity =  1731.9895949159693\n",
      "for k =  0.36000000000000015  perplexity =  1731.342167719881\n",
      "for k =  0.37000000000000016  perplexity =  1730.7235828466019\n",
      "for k =  0.38000000000000017  perplexity =  1730.1323629711155\n",
      "for k =  0.3900000000000002  perplexity =  1729.5671450898735\n",
      "for k =  0.4000000000000002  perplexity =  1729.026668925287\n",
      "for k =  0.4100000000000002  perplexity =  1728.5097667672553\n",
      "for k =  0.4200000000000002  perplexity =  1728.0153545422345\n",
      "for k =  0.4300000000000002  perplexity =  1727.5424239359138\n",
      "for k =  0.4400000000000002  perplexity =  1727.0900354243743\n",
      "for k =  0.45000000000000023  perplexity =  1726.657312089537\n",
      "for k =  0.46000000000000024  perplexity =  1726.243434117258\n",
      "for k =  0.47000000000000025  perplexity =  1725.8476338875275\n",
      "for k =  0.48000000000000026  perplexity =  1725.4691915830765\n",
      "for k =  0.49000000000000027  perplexity =  1725.107431251793\n",
      "for k =  0.5000000000000002  perplexity =  1724.7617172676962\n",
      "for k =  0.5100000000000002  perplexity =  1724.431451142923\n",
      "for k =  0.5200000000000002  perplexity =  1724.1160686495637\n",
      "for k =  0.5300000000000002  perplexity =  1723.8150372168734\n",
      "for k =  0.5400000000000003  perplexity =  1723.5278535711514\n",
      "for k =  0.5500000000000003  perplexity =  1723.2540415928352\n",
      "for k =  0.5600000000000003  perplexity =  1722.99315036735\n",
      "for k =  0.5700000000000003  perplexity =  1722.744752407615\n",
      "for k =  0.5800000000000003  perplexity =  1722.5084420322917\n",
      "for k =  0.5900000000000003  perplexity =  1722.2838338819374\n",
      "for k =  0.6000000000000003  perplexity =  1722.070561560473\n",
      "for k =  0.6100000000000003  perplexity =  1721.8682763885963\n",
      "for k =  0.6200000000000003  perplexity =  1721.6766462587318\n",
      "for k =  0.6300000000000003  perplexity =  1721.4953545814685\n",
      "for k =  0.6400000000000003  perplexity =  1721.3240993152701\n",
      "for k =  0.6500000000000004  perplexity =  1721.1625920706588\n",
      "for k =  0.6600000000000004  perplexity =  1721.0105572836444\n",
      "for k =  0.6700000000000004  perplexity =  1720.8677314503916\n",
      "for k =  0.6800000000000004  perplexity =  1720.7338624190318\n",
      "for k =  0.6900000000000004  perplexity =  1720.6087087333003\n",
      "for k =  0.7000000000000004  perplexity =  1720.4920390221869\n",
      "for k =  0.7100000000000004  perplexity =  1720.3836314344712\n",
      "for k =  0.7200000000000004  perplexity =  1720.2832731113729\n",
      "for k =  0.7300000000000004  perplexity =  1720.1907596959538\n",
      "for k =  0.7400000000000004  perplexity =  1720.1058948759626\n",
      "for k =  0.7500000000000004  perplexity =  1720.028489957195\n",
      "for k =  0.7600000000000005  perplexity =  1719.9583634647142\n",
      "for k =  0.7700000000000005  perplexity =  1719.8953407706795\n",
      "for k =  0.7800000000000005  perplexity =  1719.8392537451532\n",
      "for k =  0.7900000000000005  perplexity =  1719.7899404301536\n",
      "for k =  0.8000000000000005  perplexity =  1719.747244733497\n",
      "for k =  0.8100000000000005  perplexity =  1719.7110161415674\n",
      "for k =  0.8200000000000005  perplexity =  1719.6811094497275\n",
      "for k =  0.8300000000000005  perplexity =  1719.6573845089802\n",
      "for k =  0.8400000000000005  perplexity =  1719.639705988208\n",
      "for k =  0.8500000000000005  perplexity =  1719.6279431492737\n",
      "for k =  0.8600000000000005  perplexity =  1719.6219696364442\n",
      "for k =  0.8700000000000006  perplexity =  1719.6216632767441\n",
      "for k =  0.8800000000000006  perplexity =  1719.6269058930966\n",
      "for k =  0.8900000000000006  perplexity =  1719.6375831262067\n",
      "for k =  0.9000000000000006  perplexity =  1719.6535842678563\n",
      "for k =  0.9100000000000006  perplexity =  1719.6748021024737\n",
      "for k =  0.9200000000000006  perplexity =  1719.7011327575997\n",
      "for k =  0.9300000000000006  perplexity =  1719.732475562158\n",
      "for k =  0.9400000000000006  perplexity =  1719.7687329128548\n",
      "for k =  0.9500000000000006  perplexity =  1719.8098101464254\n",
      "for k =  0.9600000000000006  perplexity =  1719.8556154199805\n",
      "for k =  0.9700000000000006  perplexity =  1719.9060595961914\n",
      "for k =  0.9800000000000006  perplexity =  1719.961056135167\n",
      "for k =  0.9900000000000007  perplexity =  1720.0205209914136\n",
      "for k =  1.0000000000000007  perplexity =  1720.0843725157401\n",
      "optimal value of k =  0.8700000000000006\n",
      "best perpelxity using optimal k =  1720.2811879081596\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def unigram_model(train_sentences,test_sentences,k):\n",
    "    # dictionary of unigram\n",
    "    unigram_dictionary = {}\n",
    "    total_train_words=0\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        for token in words_sentence:\n",
    "            total_train_words+=1\n",
    "            unigram_dictionary[token] = unigram_dictionary.get(token,0) + 1\n",
    "\n",
    "    vocabulary = len(unigram_dictionary)\n",
    "  \n",
    "    # unigram probability distribution\n",
    "    unigram_probability = {}\n",
    "    for key,value in unigram_dictionary.items():\n",
    "        unigram_probability[key] = (value+k)/(total_train_words+k*vocabulary)\n",
    "    \n",
    "    # perplexity evaluation\n",
    "    log_probability =0.0\n",
    "    total_test_word=0\n",
    "    constant_probability = k/(total_train_words+k*vocabulary)\n",
    "    for sentence in test_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        probability_sentence=0.0\n",
    "        for token in words_sentence:\n",
    "            total_test_word+=1\n",
    "            if token in unigram_probability:\n",
    "                probability_sentence += math.log(unigram_probability[token],2)\n",
    "            else:\n",
    "                probability_sentence +=math.log(constant_probability)\n",
    "        log_probability  += probability_sentence\n",
    "    perlexity = 2**(-log_probability/total_test_word)\n",
    "    return perlexity\n",
    "\n",
    "# access the training and testing datas and spliting into sentences\n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "with open(\"brown.dev.txt\", 'r') as file:\n",
    "    dev_text_corpus = file.read()\n",
    "\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "dev_text_corpus = re.sub(r'[^\\w\\s]', '', dev_text_corpus)\n",
    "dev_sentences =dev_text_corpus.splitlines()\n",
    "\n",
    "# k smoothing - k ranges from 0 to 1 with 0.01 scale\n",
    "# brute force approach\n",
    "k =0\n",
    "min_perplexity =float(\"inf\")\n",
    "optimal_k=0\n",
    "while k<=1:\n",
    "    k=k+0.01\n",
    "    perplexity = unigram_model(train_sentences,dev_sentences,k)\n",
    "    print(\"for k = \",k,\" perplexity = \",perplexity)\n",
    "    if perplexity<min_perplexity:\n",
    "        min_perplexity=perplexity\n",
    "        optimal_k=k\n",
    "perplexity = unigram_model(train_sentences,test_sentences,optimal_k)\n",
    "print(\"optimal value of k = \",optimal_k)\n",
    "print(\"best perpelxity using optimal k = \",perplexity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6546350d-8444-4152-8105-6eb7d58bba72",
   "metadata": {},
   "source": [
    "For calcualting optimal values of k, we are checking for each value ranging from 0 to 1 with a scale of 0.01 and checking the perpelxity for each k and retrieving the optimal value of k which gives the minimum perplexity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc7074cf-406b-40af-87b0-b597ec71b0d0",
   "metadata": {},
   "source": [
    "This is the way to implement the K smoothing and same approach has be done in rest two models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6296a2cb-22f0-449e-a2c7-a617bcce7eed",
   "metadata": {},
   "source": [
    "The comapring perplexity is calculated using the dev text and at the end using the optimal found out k we get the best perplexity for test text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8d9c6e5-5242-4bf3-a008-2e519e58959e",
   "metadata": {},
   "source": [
    "The code builds an unigram language model with smoothing (k-smoothing) and calculates perplexity for text data. In the training phase, it processes training text, constructs a dictionary of unigrams, and calculates unigram probabilities with smoothing (k). In the testing phase, it evaluates the unigram model on test text, calculates the perplexity using the optimal k value found during training, and prints the optimal k value and the resulting perplexity score. The code also iterates through different k values to find the one that minimizes perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da1712-6c66-4d6b-b954-1809a1f4c8ca",
   "metadata": {},
   "source": [
    "**Smoothing in bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3747a3e2-e270-4bd9-9379-aa2b175ea89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k =  0.01  perplexity =  1224.4202351138078\n",
      "for k =  0.02  perplexity =  1358.6484724899105\n",
      "for k =  0.03  perplexity =  1471.1147864702402\n",
      "for k =  0.04  perplexity =  1569.399193114034\n",
      "for k =  0.05  perplexity =  1657.8240745088046\n",
      "for k =  0.060000000000000005  perplexity =  1738.9191020316355\n",
      "for k =  0.07  perplexity =  1814.2939798879092\n",
      "for k =  0.08  perplexity =  1885.0433024281326\n",
      "for k =  0.09  perplexity =  1951.9500739665257\n",
      "for k =  0.09999999999999999  perplexity =  2015.596893725242\n",
      "for k =  0.10999999999999999  perplexity =  2076.4310291024644\n",
      "for k =  0.11999999999999998  perplexity =  2134.804691517747\n",
      "for k =  0.12999999999999998  perplexity =  2191.001132265915\n",
      "for k =  0.13999999999999999  perplexity =  2245.2522057761266\n",
      "for k =  0.15  perplexity =  2297.7505744424943\n",
      "for k =  0.16  perplexity =  2348.658424261617\n",
      "for k =  0.17  perplexity =  2398.1138367825997\n",
      "for k =  0.18000000000000002  perplexity =  2446.2355438722466\n",
      "for k =  0.19000000000000003  perplexity =  2493.126540037342\n",
      "for k =  0.20000000000000004  perplexity =  2538.8768707742747\n",
      "for k =  0.21000000000000005  perplexity =  2583.5658155946835\n",
      "for k =  0.22000000000000006  perplexity =  2627.2636189819364\n",
      "for k =  0.23000000000000007  perplexity =  2670.032878700383\n",
      "for k =  0.24000000000000007  perplexity =  2711.9296708992415\n",
      "for k =  0.25000000000000006  perplexity =  2753.0044705644596\n",
      "for k =  0.26000000000000006  perplexity =  2793.3029110765146\n",
      "for k =  0.2700000000000001  perplexity =  2832.8664159852287\n",
      "for k =  0.2800000000000001  perplexity =  2871.7327283479162\n",
      "for k =  0.2900000000000001  perplexity =  2909.936357240758\n",
      "for k =  0.3000000000000001  perplexity =  2947.508956766975\n",
      "for k =  0.3100000000000001  perplexity =  2984.479649638494\n",
      "for k =  0.3200000000000001  perplexity =  3020.8753049374654\n",
      "for k =  0.3300000000000001  perplexity =  3056.7207777538924\n",
      "for k =  0.34000000000000014  perplexity =  3092.039116912957\n",
      "for k =  0.35000000000000014  perplexity =  3126.8517458422575\n",
      "for k =  0.36000000000000015  perplexity =  3161.1786207108303\n",
      "for k =  0.37000000000000016  perplexity =  3195.0383692405217\n",
      "for k =  0.38000000000000017  perplexity =  3228.448413005201\n",
      "for k =  0.3900000000000002  perplexity =  3261.4250755595126\n",
      "for k =  0.4000000000000002  perplexity =  3293.9836783569986\n",
      "for k =  0.4100000000000002  perplexity =  3326.1386261046973\n",
      "for k =  0.4200000000000002  perplexity =  3357.9034829444527\n",
      "for k =  0.4300000000000002  perplexity =  3389.291040640398\n",
      "for k =  0.4400000000000002  perplexity =  3420.3133797755672\n",
      "for k =  0.45000000000000023  perplexity =  3450.981924816396\n",
      "for k =  0.46000000000000024  perplexity =  3481.307493782399\n",
      "for k =  0.47000000000000025  perplexity =  3511.3003431532034\n",
      "for k =  0.48000000000000026  perplexity =  3540.9702085601543\n",
      "for k =  0.49000000000000027  perplexity =  3570.3263417408875\n",
      "for k =  0.5000000000000002  perplexity =  3599.377544164006\n",
      "for k =  0.5100000000000002  perplexity =  3628.132197689485\n",
      "for k =  0.5200000000000002  perplexity =  3656.598292575171\n",
      "for k =  0.5300000000000002  perplexity =  3684.7834531063013\n",
      "for k =  0.5400000000000003  perplexity =  3712.694961093728\n",
      "for k =  0.5500000000000003  perplexity =  3740.339777449177\n",
      "for k =  0.5600000000000003  perplexity =  3767.724562031777\n",
      "for k =  0.5700000000000003  perplexity =  3794.8556919302687\n",
      "for k =  0.5800000000000003  perplexity =  3821.7392783293153\n",
      "for k =  0.5900000000000003  perplexity =  3848.3811820932624\n",
      "for k =  0.6000000000000003  perplexity =  3874.787028185217\n",
      "for k =  0.6100000000000003  perplexity =  3900.962219025663\n",
      "for k =  0.6200000000000003  perplexity =  3926.9119468832578\n",
      "for k =  0.6300000000000003  perplexity =  3952.6412053868125\n",
      "for k =  0.6400000000000003  perplexity =  3978.1548002295845\n",
      "for k =  0.6500000000000004  perplexity =  4003.4573591361527\n",
      "for k =  0.6600000000000004  perplexity =  4028.55334115357\n",
      "for k =  0.6700000000000004  perplexity =  4053.4470453204426\n",
      "for k =  0.6800000000000004  perplexity =  4078.142618768539\n",
      "for k =  0.6900000000000004  perplexity =  4102.644064294376\n",
      "for k =  0.7000000000000004  perplexity =  4126.955247452482\n",
      "for k =  0.7100000000000004  perplexity =  4151.07990319727\n",
      "for k =  0.7200000000000004  perplexity =  4175.021642116171\n",
      "for k =  0.7300000000000004  perplexity =  4198.783956279071\n",
      "for k =  0.7400000000000004  perplexity =  4222.370224735905\n",
      "for k =  0.7500000000000004  perplexity =  4245.7837186861725\n",
      "for k =  0.7600000000000005  perplexity =  4269.027606345739\n",
      "for k =  0.7700000000000005  perplexity =  4292.104957531127\n",
      "for k =  0.7800000000000005  perplexity =  4315.018747981605\n",
      "for k =  0.7900000000000005  perplexity =  4337.771863436834\n",
      "for k =  0.8000000000000005  perplexity =  4360.3671034890285\n",
      "for k =  0.8100000000000005  perplexity =  4382.807185221289\n",
      "for k =  0.8200000000000005  perplexity =  4405.09474664886\n",
      "for k =  0.8300000000000005  perplexity =  4427.232349976904\n",
      "for k =  0.8400000000000005  perplexity =  4449.222484683864\n",
      "for k =  0.8500000000000005  perplexity =  4471.06757044364\n",
      "for k =  0.8600000000000005  perplexity =  4492.769959899274\n",
      "for k =  0.8700000000000006  perplexity =  4514.331941289582\n",
      "for k =  0.8800000000000006  perplexity =  4535.755740949075\n",
      "for k =  0.8900000000000006  perplexity =  4557.043525678323\n",
      "for k =  0.9000000000000006  perplexity =  4578.197405000003\n",
      "for k =  0.9100000000000006  perplexity =  4599.219433303149\n",
      "for k =  0.9200000000000006  perplexity =  4620.111611885262\n",
      "for k =  0.9300000000000006  perplexity =  4640.875890896079\n",
      "for k =  0.9400000000000006  perplexity =  4661.514171189434\n",
      "for k =  0.9500000000000006  perplexity =  4682.028306089946\n",
      "for k =  0.9600000000000006  perplexity =  4702.420103075989\n",
      "for k =  0.9700000000000006  perplexity =  4722.691325388301\n",
      "for k =  0.9800000000000006  perplexity =  4742.843693562538\n",
      "for k =  0.9900000000000007  perplexity =  4762.878886896592\n",
      "for k =  1.0000000000000007  perplexity =  4782.798544851218\n",
      "optimal value of k =  0.01\n",
      "best perpelxity using optimal k =  1209.6646464950297\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def bigram_model(train_sentences,test_sentences,k):\n",
    "    # dictionary of unigram model\n",
    "    unigram_dictionary = {}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        for token in words_sentence:\n",
    "            unigram_dictionary[token] = unigram_dictionary.get(token,0) + 1\n",
    "    \n",
    "    # dictionary of bigram model\n",
    "    bigram_dictionary={}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence =sentence.split()\n",
    "        len_sen = len(words_sentence)\n",
    "        for i in range(len_sen-1):\n",
    "            bigram =[]\n",
    "            for j in range(2):\n",
    "                bigram.append(words_sentence[i+j])\n",
    "            bigram_tuple=tuple(bigram)\n",
    "            bigram_dictionary[bigram_tuple]=bigram_dictionary.get(bigram_tuple,0) +1\n",
    "    \n",
    "    vocabulary = len(unigram_dictionary)\n",
    "\n",
    "    # bigram probability distribution\n",
    "    bigram_probability={}\n",
    "    for key,value in bigram_dictionary.items():\n",
    "        bigram_probability[key]=(value+k)/(unigram_dictionary[key[0]]+k*vocabulary)\n",
    "\n",
    "    # perflexity evaluation\n",
    "    log_probability =0.0\n",
    "    total_test_words=0\n",
    "    for sentence in test_sentences:\n",
    "        words_sentence = sentence.split()\n",
    "        probability_sentence=0.0\n",
    "        len_sen = len(words_sentence)\n",
    "        total_test_words+=len_sen\n",
    "        for i in range(len_sen-1):\n",
    "            bigram =[]\n",
    "            for j in range(2):\n",
    "                bigram.append(words_sentence[i+j])\n",
    "            bigram_tuple=tuple(bigram)\n",
    "            if bigram_tuple in bigram_dictionary:\n",
    "                probability_sentence += math.log(bigram_probability[bigram_tuple],2)\n",
    "            else:\n",
    "                probability_sentence+=math.log((k/(unigram_dictionary.get(bigram_tuple[0],0)+k*vocabulary)),2)\n",
    "        log_probability+=probability_sentence\n",
    "    perlexity = 2**(-log_probability/total_test_words)\n",
    "    return perlexity\n",
    "\n",
    "# access the training and testing datas and spliting into sentences\n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "with open(\"brown.dev.txt\", 'r') as file:\n",
    "    dev_text_corpus = file.read()\n",
    "\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "dev_text_corpus = re.sub(r'[^\\w\\s]', '', dev_text_corpus)\n",
    "dev_sentences =dev_text_corpus.splitlines()\n",
    "\n",
    "# add start and stop symbols\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = \"<s> \" + train_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = \"<s> \" + test_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(dev_sentences)):\n",
    "    dev_sentences[i] = \"<s> \" + dev_sentences[i] + \" </s>\"\n",
    "\n",
    "# k smoothing - k ranges from 0 to 1 with 0.01 scale\n",
    "# brute force approach\n",
    "k =0\n",
    "min_perplexity =float(\"inf\")\n",
    "optimal_k=0\n",
    "while k<=1:\n",
    "    k=k+0.01\n",
    "    perplexity = bigram_model(train_sentences,dev_sentences,k)\n",
    "    print(\"for k = \",k,\" perplexity = \",perplexity)\n",
    "    if perplexity<min_perplexity:\n",
    "        min_perplexity=perplexity\n",
    "        optimal_k=k\n",
    "perplexity = bigram_model(train_sentences,test_sentences,optimal_k)\n",
    "print(\"optimal value of k = \",optimal_k)\n",
    "print(\"best perpelxity using optimal k = \",perplexity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4e9e79e-af5c-4619-a361-f9b4c9812cbf",
   "metadata": {},
   "source": [
    "Same thing has been done in bigram model and it gives us the best perplexity among the three models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69fc0c06-a0cf-455f-83d9-3904ae198cef",
   "metadata": {},
   "source": [
    "In order to calculate the probability of a sentence in bigram, both bigram and trigram distribution need to know as P(wi/wi-1)=P(wi-1,wi)/P(wi-1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf2f5b94-a948-4637-a19d-a9e58d8b46cb",
   "metadata": {},
   "source": [
    "The code is designed to create a bigram language model with smoothing (k-smoothing) and compute perplexity for text data. During training, it processes training text, constructs dictionaries for unigrams and bigrams, and calculates bigram probabilities with smoothing (k). During testing, it assesses the bigram model on test text, computes perplexity using the optimal k value determined during training, and displays the optimal k value along with the resulting perplexity score. The code employs a systematic search over various k values to identify the one that minimizes perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a2f77-e5c5-4127-a983-ad7c3436b1d7",
   "metadata": {},
   "source": [
    "**Smoothing in trigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b84e330-c4c4-4c57-b597-464a9458c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k =  0.01  perplexity =  5391.367574129816\n",
      "for k =  0.02  perplexity =  5942.757584149506\n",
      "for k =  0.03  perplexity =  6332.036376693959\n",
      "for k =  0.04  perplexity =  6639.17766664029\n",
      "for k =  0.05  perplexity =  6895.408414831025\n",
      "for k =  0.060000000000000005  perplexity =  7116.514645516126\n",
      "for k =  0.07  perplexity =  7311.70364079536\n",
      "for k =  0.08  perplexity =  7486.86378168535\n",
      "for k =  0.09  perplexity =  7646.0125384130115\n",
      "for k =  0.09999999999999999  perplexity =  7792.025893859394\n",
      "for k =  0.10999999999999999  perplexity =  7927.040694872108\n",
      "for k =  0.11999999999999998  perplexity =  8052.692417377338\n",
      "for k =  0.12999999999999998  perplexity =  8170.263501605174\n",
      "for k =  0.13999999999999999  perplexity =  8280.780046394491\n",
      "for k =  0.15  perplexity =  8385.077170114813\n",
      "for k =  0.16  perplexity =  8483.844561495218\n",
      "for k =  0.17  perplexity =  8577.659062796669\n",
      "for k =  0.18000000000000002  perplexity =  8667.008507792596\n",
      "for k =  0.19000000000000003  perplexity =  8752.309507910755\n",
      "for k =  0.20000000000000004  perplexity =  8833.920954656882\n",
      "for k =  0.21000000000000005  perplexity =  8912.154428748494\n",
      "for k =  0.22000000000000006  perplexity =  8987.282335557938\n",
      "for k =  0.23000000000000007  perplexity =  9059.54434250624\n",
      "for k =  0.24000000000000007  perplexity =  9129.152530007896\n",
      "for k =  0.25000000000000006  perplexity =  9196.295555065399\n",
      "for k =  0.26000000000000006  perplexity =  9261.142048070273\n",
      "for k =  0.2700000000000001  perplexity =  9323.843407624137\n",
      "for k =  0.2800000000000001  perplexity =  9384.536118078571\n",
      "for k =  0.2900000000000001  perplexity =  9443.34368516982\n",
      "for k =  0.3000000000000001  perplexity =  9500.37826350402\n",
      "for k =  0.3100000000000001  perplexity =  9555.742033428962\n",
      "for k =  0.3200000000000001  perplexity =  9609.52837261504\n",
      "for k =  0.3300000000000001  perplexity =  9661.822858328464\n",
      "for k =  0.34000000000000014  perplexity =  9712.704129178144\n",
      "for k =  0.35000000000000014  perplexity =  9762.244629537628\n",
      "for k =  0.36000000000000015  perplexity =  9810.511255476878\n",
      "for k =  0.37000000000000016  perplexity =  9857.565917558944\n",
      "for k =  0.38000000000000017  perplexity =  9903.466033142864\n",
      "for k =  0.3900000000000002  perplexity =  9948.2649586122\n",
      "for k =  0.4000000000000002  perplexity =  9992.012370205399\n",
      "for k =  0.4100000000000002  perplexity =  10034.754600654853\n",
      "for k =  0.4200000000000002  perplexity =  10076.534937732678\n",
      "for k =  0.4300000000000002  perplexity =  10117.393889772899\n",
      "for k =  0.4400000000000002  perplexity =  10157.369422523365\n",
      "for k =  0.45000000000000023  perplexity =  10196.497170983848\n",
      "for k =  0.46000000000000024  perplexity =  10234.810629364056\n",
      "for k =  0.47000000000000025  perplexity =  10272.341321841795\n",
      "for k =  0.48000000000000026  perplexity =  10309.118956426179\n",
      "for k =  0.49000000000000027  perplexity =  10345.171563911339\n",
      "for k =  0.5000000000000002  perplexity =  10380.525623628591\n",
      "for k =  0.5100000000000002  perplexity =  10415.206177498805\n",
      "for k =  0.5200000000000002  perplexity =  10449.236933668573\n",
      "for k =  0.5300000000000002  perplexity =  10482.64036086508\n",
      "for k =  0.5400000000000003  perplexity =  10515.437774461201\n",
      "for k =  0.5500000000000003  perplexity =  10547.649415108894\n",
      "for k =  0.5600000000000003  perplexity =  10579.294520713518\n",
      "for k =  0.5700000000000003  perplexity =  10610.39139241389\n",
      "for k =  0.5800000000000003  perplexity =  10640.95745516549\n",
      "for k =  0.5900000000000003  perplexity =  10671.009313449696\n",
      "for k =  0.6000000000000003  perplexity =  10700.562802583136\n",
      "for k =  0.6100000000000003  perplexity =  10729.633036025725\n",
      "for k =  0.6200000000000003  perplexity =  10758.234449080948\n",
      "for k =  0.6300000000000003  perplexity =  10786.380839297799\n",
      "for k =  0.6400000000000003  perplexity =  10814.085403877798\n",
      "for k =  0.6500000000000004  perplexity =  10841.360774353405\n",
      "for k =  0.6600000000000004  perplexity =  10868.219048773757\n",
      "for k =  0.6700000000000004  perplexity =  10894.671821614005\n",
      "for k =  0.6800000000000004  perplexity =  10920.730211588176\n",
      "for k =  0.6900000000000004  perplexity =  10946.404887560428\n",
      "for k =  0.7000000000000004  perplexity =  10971.706092697133\n",
      "for k =  0.7100000000000004  perplexity =  10996.643666996946\n",
      "for k =  0.7200000000000004  perplexity =  11021.227068350378\n",
      "for k =  0.7300000000000004  perplexity =  11045.465392216065\n",
      "for k =  0.7400000000000004  perplexity =  11069.367390043828\n",
      "for k =  0.7500000000000004  perplexity =  11092.941486533042\n",
      "for k =  0.7600000000000005  perplexity =  11116.195795801965\n",
      "for k =  0.7700000000000005  perplexity =  11139.138136576394\n",
      "for k =  0.7800000000000005  perplexity =  11161.77604644277\n",
      "for k =  0.7900000000000005  perplexity =  11184.116795244845\n",
      "for k =  0.8000000000000005  perplexity =  11206.16739769497\n",
      "for k =  0.8100000000000005  perplexity =  11227.934625233987\n",
      "for k =  0.8200000000000005  perplexity =  11249.425017210553\n",
      "for k =  0.8300000000000005  perplexity =  11270.644891424334\n",
      "for k =  0.8400000000000005  perplexity =  11291.600354065418\n",
      "for k =  0.8500000000000005  perplexity =  11312.297309103164\n",
      "for k =  0.8600000000000005  perplexity =  11332.74146715399\n",
      "for k =  0.8700000000000006  perplexity =  11352.938353868169\n",
      "for k =  0.8800000000000006  perplexity =  11372.893317860908\n",
      "for k =  0.8900000000000006  perplexity =  11392.611538221598\n",
      "for k =  0.9000000000000006  perplexity =  11412.098031624026\n",
      "for k =  0.9100000000000006  perplexity =  11431.357659070141\n",
      "for k =  0.9200000000000006  perplexity =  11450.395132279591\n",
      "for k =  0.9300000000000006  perplexity =  11469.215019756959\n",
      "for k =  0.9400000000000006  perplexity =  11487.82175255093\n",
      "for k =  0.9500000000000006  perplexity =  11506.219629722298\n",
      "for k =  0.9600000000000006  perplexity =  11524.41282354744\n",
      "for k =  0.9700000000000006  perplexity =  11542.405384461352\n",
      "for k =  0.9800000000000006  perplexity =  11560.201245759019\n",
      "for k =  0.9900000000000007  perplexity =  11577.804228080448\n",
      "for k =  1.0000000000000007  perplexity =  11595.218043670184\n",
      "optimal value of k =  0.01\n",
      "best perpelxity using optimal k =  5392.758146622303\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def trigram_model(train_sentences,test_sentences,k):\n",
    "    # dictionary of unigram model\n",
    "    unigram_dictionary = {}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        for token in words_sentence:\n",
    "            unigram_dictionary[token] = unigram_dictionary.get(token,0) + 1\n",
    "    \n",
    "    # dictionary of bigram model\n",
    "    bigram_dictionary={}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence =sentence.split()\n",
    "        len_sen = len(words_sentence)\n",
    "        for i in range(len_sen-1):\n",
    "            bigram =[]\n",
    "            for j in range(2):\n",
    "                bigram.append(words_sentence[i+j])\n",
    "            bigram_tuple=tuple(bigram)\n",
    "            bigram_dictionary[bigram_tuple]=bigram_dictionary.get(bigram_tuple,0) +1\n",
    "    \n",
    "    # dictionary of trigram model\n",
    "    trigram_dictionary={}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence =sentence.split()\n",
    "        len_sen = len(words_sentence)\n",
    "        for i in range(len_sen-2):\n",
    "            trigram =[]\n",
    "            for j in range(3):\n",
    "                trigram.append(words_sentence[i+j])\n",
    "            trigram_tuple=tuple(trigram)\n",
    "            trigram_dictionary[trigram_tuple]=trigram_dictionary.get(trigram_tuple,0) +1\n",
    "\n",
    "    vocabulary = len(unigram_dictionary)\n",
    "    \n",
    "    # trigram probability distribution\n",
    "    trigram_probability={}\n",
    "    for key,value in trigram_dictionary.items():\n",
    "        trigram_probability[key] =(value+k)/(bigram_dictionary[key[0:2]]+vocabulary*k)\n",
    "    \n",
    "    \n",
    "    # perflexity evaluation\n",
    "    log_probability =0.0\n",
    "    total_test_words=0\n",
    "    for sentence in test_sentences:\n",
    "        words_sentence = sentence.split()\n",
    "        probability_sentence=0.0\n",
    "        len_sen = len(words_sentence)\n",
    "        total_test_words+=len_sen\n",
    "        for i in range(len_sen-2):\n",
    "            trigram =[]\n",
    "            for j in range(3):\n",
    "                trigram.append(words_sentence[i+j])\n",
    "                trigram_tuple=tuple(trigram)\n",
    "            bigram_tuple=trigram_tuple[0:2]\n",
    "            if trigram_tuple in trigram_dictionary:\n",
    "                probability_sentence += math.log(trigram_probability[trigram_tuple],2)\n",
    "            else:\n",
    "                probability_sentence+=math.log((k/(bigram_dictionary.get(bigram_tuple,0)+k*vocabulary)),2)\n",
    "                \n",
    "        log_probability+=probability_sentence\n",
    "    perlexity = 2**(-log_probability/total_test_words)\n",
    "    return perlexity\n",
    "\n",
    "\n",
    "\n",
    "# access the training and testing datas and spliting into sentences\n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "with open(\"brown.dev.txt\", 'r') as file:\n",
    "    dev_text_corpus = file.read()\n",
    "\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "dev_text_corpus = re.sub(r'[^\\w\\s]', '', dev_text_corpus)\n",
    "dev_sentences =dev_text_corpus.splitlines()\n",
    "\n",
    "# add start and stop symbols\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = \"<s> \" + train_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = \"<s> \" + test_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(dev_sentences)):\n",
    "    dev_sentences[i] = \"<s> \" + dev_sentences[i] + \" </s>\"\n",
    "\n",
    "# k smoothing - k ranges from 0 to 1 with 0.01 scale\n",
    "# brute force approach\n",
    "k =0\n",
    "min_perplexity =float(\"inf\")\n",
    "optimal_k=0\n",
    "while k<=1:\n",
    "    k=k+0.01\n",
    "    perplexity = trigram_model(train_sentences,dev_sentences,k)\n",
    "    print(\"for k = \",k,\" perplexity = \",perplexity)\n",
    "    if perplexity<min_perplexity:\n",
    "        min_perplexity=perplexity\n",
    "        optimal_k=k\n",
    "perplexity = trigram_model(train_sentences,test_sentences,optimal_k)\n",
    "print(\"optimal value of k = \",optimal_k)\n",
    "print(\"best perpelxity using optimal k = \",perplexity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7d2d5b7-90ee-4b7c-b888-7d8df1030037",
   "metadata": {},
   "source": [
    "Smoothing applied in Trigram Interpolation gives us the worst perplexity value among all three models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d006bf7c-d7af-4e10-b060-67a4fab7c465",
   "metadata": {},
   "source": [
    "In order to calculate the probability of a sentence in trigram, both bigram and trigram distribution need to know as P(wi/wi-1,wi2)=P(wi-2,wi-1,wi)/P(wi-1,wi-2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f9dcf2a-42ae-455f-8561-c5b58a290604",
   "metadata": {},
   "source": [
    "The code implements a trigram language model with smoothing (k-smoothing) and computes perplexity for text data. It follows a similar structure to the previous models but extends it to trigrams. During training, it processes training text, constructs dictionaries for unigrams, bigrams, and trigrams, calculates trigram probabilities with smoothing (k), and evaluates perplexity. During testing, it assesses the trigram model on test text using the optimal k value determined during training and displays the optimal k value along with the resulting perplexity score. The code employs a systematic search over various k values to identify the one that minimizes perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be612801-f554-41dc-b489-af349a2f5a20",
   "metadata": {},
   "source": [
    "**Linear Interpolation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd72dc5-cb60-444e-916d-f70ec80f0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values =  0.0 0.0 1.0\n",
      "Corresponding perplexity using devtext =  703.7103244402572\n",
      "Parameter values =  0.0 0.1 0.9\n",
      "Corresponding perplexity using devtext =  383.1203935243161\n",
      "Parameter values =  0.0 0.2 0.8\n",
      "Corresponding perplexity using devtext =  328.9917619391723\n",
      "Parameter values =  0.0 0.3 0.7\n",
      "Corresponding perplexity using devtext =  303.9136328252318\n",
      "Parameter values =  0.0 0.4 0.6\n",
      "Corresponding perplexity using devtext =  291.57767167404336\n",
      "Parameter values =  0.0 0.5 0.5\n",
      "Corresponding perplexity using devtext =  287.39232477238016\n",
      "Parameter values =  0.0 0.6 0.4\n",
      "Corresponding perplexity using devtext =  290.1984005861523\n",
      "Parameter values =  0.0 0.7 0.3\n",
      "Corresponding perplexity using devtext =  300.9977073546089\n",
      "Parameter values =  0.0 0.8 0.2\n",
      "Corresponding perplexity using devtext =  324.17885631387315\n",
      "Parameter values =  0.0 0.9 0.1\n",
      "Corresponding perplexity using devtext =  375.965876373091\n",
      "Parameter values =  0.0 1.0 0.0\n",
      "Corresponding perplexity using devtext =  938.6080639187057\n",
      "Parameter values =  0.1 0.0 0.9\n",
      "Corresponding perplexity using devtext =  633.0111956764435\n",
      "Parameter values =  0.1 0.1 0.8\n",
      "Corresponding perplexity using devtext =  381.67750499325524\n",
      "Parameter values =  0.1 0.2 0.7\n",
      "Corresponding perplexity using devtext =  333.89554482062397\n",
      "Parameter values =  0.1 0.3 0.6\n",
      "Corresponding perplexity using devtext =  312.39352929401014\n",
      "Parameter values =  0.1 0.4 0.5\n",
      "Corresponding perplexity using devtext =  303.35142265891875\n",
      "Parameter values =  0.1 0.5 0.4\n",
      "Corresponding perplexity using devtext =  303.2061237585696\n",
      "Parameter values =  0.1 0.6 0.3\n",
      "Corresponding perplexity using devtext =  312.0461974848138\n",
      "Parameter values =  0.1 0.7 0.2\n",
      "Corresponding perplexity using devtext =  333.793132227498\n",
      "Parameter values =  0.1 0.8 0.1\n",
      "Corresponding perplexity using devtext =  384.12088579179266\n",
      "Parameter values =  0.1 0.9 0.0\n",
      "Corresponding perplexity using devtext =  848.1211348722377\n",
      "Parameter values =  0.2 0.0 0.8\n",
      "Corresponding perplexity using devtext =  630.1032077681642\n",
      "Parameter values =  0.2 0.1 0.7\n",
      "Corresponding perplexity using devtext =  388.2495408353147\n",
      "Parameter values =  0.2 0.2 0.6\n",
      "Corresponding perplexity using devtext =  343.9877483951151\n",
      "Parameter values =  0.2 0.3 0.5\n",
      "Corresponding perplexity using devtext =  325.7135340185628\n",
      "Parameter values =  0.2 0.4 0.4\n",
      "Corresponding perplexity using devtext =  320.7249563028712\n",
      "Parameter values =  0.2 0.5 0.3\n",
      "Corresponding perplexity using devtext =  326.7290588308561\n",
      "Parameter values =  0.2 0.6 0.2\n",
      "Corresponding perplexity using devtext =  346.7868164957733\n",
      "Parameter values =  0.2 0.7 0.1\n",
      "Corresponding perplexity using devtext =  396.2555794535935\n",
      "Parameter values =  0.2 0.8 0.0\n",
      "Corresponding perplexity using devtext =  835.1759926414716\n",
      "Parameter values =  0.3 0.0 0.7\n",
      "Corresponding perplexity using devtext =  645.3442018051869\n",
      "Parameter values =  0.3 0.1 0.6\n",
      "Corresponding perplexity using devtext =  400.9539141144188\n",
      "Parameter values =  0.3 0.2 0.5\n",
      "Corresponding perplexity using devtext =  359.3840343826719\n",
      "Parameter values =  0.3 0.3 0.4\n",
      "Corresponding perplexity using devtext =  345.00768559276906\n",
      "Parameter values =  0.3 0.4 0.3\n",
      "Corresponding perplexity using devtext =  346.21875456869594\n",
      "Parameter values =  0.3 0.5 0.2\n",
      "Corresponding perplexity using devtext =  363.72367755529785\n",
      "Parameter values =  0.3 0.6 0.1\n",
      "Corresponding perplexity using devtext =  412.29080028567785\n",
      "Parameter values =  0.3 0.7 0.0\n",
      "Corresponding perplexity using devtext =  843.7706315233083\n",
      "Parameter values =  0.4 0.0 0.6\n",
      "Corresponding perplexity using devtext =  674.5907662874666\n",
      "Parameter values =  0.4 0.1 0.5\n",
      "Corresponding perplexity using devtext =  420.1364832993858\n",
      "Parameter values =  0.4 0.2 0.4\n",
      "Corresponding perplexity using devtext =  381.5220920084243\n",
      "Parameter values =  0.4 0.3 0.3\n",
      "Corresponding perplexity using devtext =  373.16705955416273\n",
      "Parameter values =  0.4 0.4 0.2\n",
      "Corresponding perplexity using devtext =  386.14231954418534\n",
      "Parameter values =  0.4 0.5 0.1\n",
      "Corresponding perplexity using devtext =  433.2034416375144\n",
      "Parameter values =  0.4 0.6 0.0\n",
      "Corresponding perplexity using devtext =  867.7579038101832\n",
      "Parameter values =  0.5 0.0 0.5\n",
      "Corresponding perplexity using devtext =  719.2273796864202\n",
      "Parameter values =  0.5 0.1 0.4\n",
      "Corresponding perplexity using devtext =  447.6740694483717\n",
      "Parameter values =  0.5 0.2 0.3\n",
      "Corresponding perplexity using devtext =  413.77719926885163\n",
      "Parameter values =  0.5 0.3 0.2\n",
      "Corresponding perplexity using devtext =  417.204952562609\n",
      "Parameter values =  0.5 0.4 0.1\n",
      "Corresponding perplexity using devtext =  461.02963662103696\n",
      "Parameter values =  0.5 0.5 0.0\n",
      "Corresponding perplexity using devtext =  907.5987370586914\n",
      "Parameter values =  0.6 0.0 0.4\n",
      "Corresponding perplexity using devtext =  784.75235736009\n",
      "Parameter values =  0.6 0.1 0.3\n",
      "Corresponding perplexity using devtext =  487.8694272376677\n",
      "Parameter values =  0.6 0.2 0.2\n",
      "Corresponding perplexity using devtext =  464.26851592327716\n",
      "Parameter values =  0.6 0.3 0.1\n",
      "Corresponding perplexity using devtext =  499.8822594583717\n",
      "Parameter values =  0.6 0.4 0.0\n",
      "Corresponding perplexity using devtext =  968.0182099227995\n",
      "Parameter values =  0.7 0.0 0.3\n",
      "Corresponding perplexity using devtext =  883.7888268813147\n",
      "Parameter values =  0.7 0.1 0.2\n",
      "Corresponding perplexity using devtext =  551.0727368818442\n",
      "Parameter values =  0.7 0.2 0.1\n",
      "Corresponding perplexity using devtext =  559.3896761657844\n",
      "Parameter values =  0.7 0.3 0.0\n",
      "Corresponding perplexity using devtext =  1060.5374790303847\n",
      "Parameter values =  0.8 0.0 0.2\n",
      "Corresponding perplexity using devtext =  1048.013457345365\n",
      "Parameter values =  0.8 0.1 0.1\n",
      "Corresponding perplexity using devtext =  671.159222144275\n",
      "Parameter values =  0.8 0.2 0.0\n",
      "Corresponding perplexity using devtext =  1214.4583241889031\n",
      "Parameter values =  0.9 0.0 0.1\n",
      "Corresponding perplexity using devtext =  1389.485391095773\n",
      "Parameter values =  0.9 0.1 0.0\n",
      "Corresponding perplexity using devtext =  1534.0132303448227\n",
      "Parameter values =  1.0 0.0 0.0\n",
      "Corresponding perplexity using devtext =  5391.367574129816\n",
      "The optimal values of l1 =  0.0  l2 =  0.5  l3 =  0.5\n",
      "Best Perplexity =  285.5124369943962\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def trigram_model_with_interpolation(train_sentences,test_sentences,l1,l2,l3):\n",
    "    k_uni = 0.87\n",
    "    k_bi = 0.01\n",
    "    k_tri = 0.01\n",
    "    # dictionary of unigram model\n",
    "    total_train_words=0\n",
    "    unigram_dictionary = {}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        for token in words_sentence:\n",
    "            total_train_words+=1\n",
    "            unigram_dictionary[token] = unigram_dictionary.get(token,0) + 1\n",
    "    \n",
    "    # dictionary of bigram model\n",
    "    bigram_dictionary={}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence =sentence.split()\n",
    "        len_sen = len(words_sentence)\n",
    "        for i in range(len_sen-1):\n",
    "            bigram =[]\n",
    "            for j in range(2):\n",
    "                bigram.append(words_sentence[i+j])\n",
    "            bigram_tuple=tuple(bigram)\n",
    "            bigram_dictionary[bigram_tuple]=bigram_dictionary.get(bigram_tuple,0) +1\n",
    "    \n",
    "    # dictionary of trigram model\n",
    "    trigram_dictionary={}\n",
    "    for sentence in train_sentences:\n",
    "        words_sentence =sentence.split()\n",
    "        len_sen = len(words_sentence)\n",
    "        for i in range(len_sen-2):\n",
    "            trigram =[]\n",
    "            for j in range(3):\n",
    "                trigram.append(words_sentence[i+j])\n",
    "            trigram_tuple=tuple(trigram)\n",
    "            trigram_dictionary[trigram_tuple]=trigram_dictionary.get(trigram_tuple,0) +1\n",
    "\n",
    "    vocabulary = len(unigram_dictionary)\n",
    "\n",
    "    # trigram probability distribution\n",
    "    trigram_probability={}\n",
    "    for key,value in trigram_dictionary.items():\n",
    "        trigram_probability[key] =(value+k_tri)/(bigram_dictionary[key[0:2]]+vocabulary*k_tri)\n",
    "    \n",
    "    # bigram probability distribution\n",
    "    bigram_probability={}\n",
    "    for key,value in bigram_dictionary.items():\n",
    "        bigram_probability[key]=(value+k_bi)/(unigram_dictionary[key[0]]+k_bi*vocabulary)\n",
    "\n",
    "    # unigram probability distribution\n",
    "    unigram_probability = {}\n",
    "    for key,value in unigram_dictionary.items():\n",
    "        unigram_probability[key] = (value+k_uni)/(total_train_words+k_uni*vocabulary)\n",
    "    \n",
    "    #probability of the dev text\n",
    "    log_probability =0.0\n",
    "    total_test_words =0\n",
    "    constant_probability = k_uni/(total_train_words+k_uni*vocabulary)\n",
    "    for sentence in  test_sentences:\n",
    "        words_sentence=sentence.split()\n",
    "        probability_sentence=0.0\n",
    "        len_sen = len(words_sentence)\n",
    "        total_test_words+=len_sen\n",
    "        for i in range(len_sen-2):\n",
    "            trigram =[]\n",
    "            for j in range(3):\n",
    "                trigram.append(words_sentence[i+j])\n",
    "            trigram_tuple=tuple(trigram)\n",
    "            bigram_tuple=trigram_tuple[0:2]\n",
    "            unigram_word =trigram_tuple[0]\n",
    "            p1=0\n",
    "            p2=0\n",
    "            p3=0\n",
    "            # probability by trigram\n",
    "            if trigram_tuple in trigram_dictionary:\n",
    "                p1 = trigram_probability[trigram_tuple]\n",
    "            else:\n",
    "                p1 =k_tri/(bigram_dictionary.get(bigram_tuple,0)+k_tri*vocabulary)\n",
    "            #probability by bigram\n",
    "            if bigram_tuple in bigram_dictionary:\n",
    "                p2 = bigram_probability[bigram_tuple]\n",
    "            else:\n",
    "                p2 = k_bi/(unigram_dictionary.get(unigram_word,0)+k_bi*vocabulary)\n",
    "            #probability by unigram\n",
    "            if unigram_word in unigram_probability:\n",
    "                p3 = unigram_probability[unigram_word]\n",
    "            else:\n",
    "                p3 = constant_probability\n",
    "            pf =math.log(l1*p1+l2*p2+l3*p3,2)\n",
    "            probability_sentence +=pf\n",
    "        log_probability+=probability_sentence\n",
    "    perlexity = 2**(-log_probability/total_test_words)\n",
    "    return perlexity\n",
    "\n",
    "\n",
    "# access the training and testing datas and spliting into sentences\n",
    "with open(\"brown.train.txt\", 'r') as file:\n",
    "    train_text_corpus = file.read()\n",
    "\n",
    "with open(\"brown.test.txt\", 'r') as file:\n",
    "    test_text_corpus = file.read()\n",
    "\n",
    "with open(\"brown.dev.txt\", 'r') as file:\n",
    "    dev_text_corpus = file.read()\n",
    "\n",
    "train_text_corpus = re.sub(r'[^\\w\\s]', '', train_text_corpus)\n",
    "train_sentences =train_text_corpus.splitlines()\n",
    "\n",
    "test_text_corpus = re.sub(r'[^\\w\\s]', '', test_text_corpus)\n",
    "test_sentences =test_text_corpus.splitlines()\n",
    "\n",
    "dev_text_corpus = re.sub(r'[^\\w\\s]', '', dev_text_corpus)\n",
    "dev_sentences =dev_text_corpus.splitlines()\n",
    "\n",
    "# add start and stop symbols\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = \"<s> \" + train_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = \"<s> \" + test_sentences[i] + \" </s>\"\n",
    "\n",
    "for i in range(len(dev_sentences)):\n",
    "    dev_sentences[i] = \"<s> \" + dev_sentences[i] + \" </s>\"\n",
    "\n",
    "# linear interpolation\n",
    "l1=0\n",
    "l2=0\n",
    "l3=0\n",
    "optimal_l1=0\n",
    "optimal_l2=0\n",
    "optimal_l3=0\n",
    "resulted_perplexity =float(\"inf\")\n",
    "for x1 in range(11):\n",
    "    for x2 in range(11-x1):\n",
    "        x3=10-x1-x2\n",
    "        l1=x1/10\n",
    "        l2=x2/10\n",
    "        l3=x3/10\n",
    "        print(\"Parameter values = \",l1,l2,l3)\n",
    "        current_perplexity =trigram_model_with_interpolation(train_sentences,dev_sentences,l1,l2,l3)\n",
    "        print(\"Corresponding perplexity using devtext = \",current_perplexity)\n",
    "        if(current_perplexity<resulted_perplexity):\n",
    "            resulted_perplexity=current_perplexity\n",
    "            optimal_l1=l1\n",
    "            optimal_l2=l2\n",
    "            optimal_l3=l3\n",
    "\n",
    "print(\"The optimal values of l1 = \",optimal_l1,\" l2 = \",optimal_l2,\" l3 = \",optimal_l3)\n",
    "resulted_perplexity =trigram_model_with_interpolation(train_sentences,test_sentences,optimal_l1,optimal_l2,optimal_l3)\n",
    "print(\"Best Perplexity = \",resulted_perplexity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7196ff39-73f0-4889-9c71-54d396fedf55",
   "metadata": {},
   "source": [
    "Linear Interpolation is basically considering the all models while calculating the probability of the text giving the effective weight to each of the unigram, bigram and trigram model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cf4890e-eccf-44e2-9350-821ffeb1e7a4",
   "metadata": {},
   "source": [
    "The code implements a trigram language model with linear interpolation smoothing and computes perplexity for text data. It follows a similar structure to the previous models but extends it to trigrams with linear interpolation of probabilities from unigram, bigram, and trigram models.\n",
    "\n",
    "The code iterates through various combinations of interpolation parameters (l1, l2, l3) to find the optimal values that minimize perplexity on a development set. It then uses these optimal values to compute perplexity on a test set.\n",
    "\n",
    "The linear interpolation helps combine the strengths of unigram, bigram, and trigram models to achieve better language modeling performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c8706fe-3091-4438-bbe6-87bf007ce018",
   "metadata": {},
   "source": [
    "So the objective of the linear interpolation is to find the optimal values of the parameters which is denoted by the symbols l1, l2 and l3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e250083d-08a3-4457-b563-195c4b134660",
   "metadata": {},
   "source": [
    "We got the best perplexity of dev text using the parameters l1 = 0, l2 = 0.5 and l3 = 0.5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "157e93da-1eb6-4cec-a7e4-8313630f3ac1",
   "metadata": {},
   "source": [
    "This suggest that giving zero weight to trigram model (that is neglect the model), 0.5 weight each to unigram and bigram model gives us the best effective perplexity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89401b68-5e16-4326-ab58-57182708440f",
   "metadata": {},
   "source": [
    "So the final selection for k smoothing,\n",
    "1. unigram - k=0.87\n",
    "2. bigram -  k=0.01\n",
    "3. trigram - k=0.01\n",
    "And the final selection for lambdas for linear interpolation,\n",
    "1. lambda1 or l1 = 0.0\n",
    "2. lambda2 or l2 = 0.5\n",
    "2. lambda3 or l3 = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
